{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c1ac08a-228f-477f-b310-e9c88220c153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align: center;margin-top:20px\"><img src=\"1.weBP\" style=\"max-width: 100%; height: 500px;\" /></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = r\"1.weBP\"\n",
    "\n",
    "display(HTML(f'<div style=\"text-align: center;margin-top:20px\"><img src=\"{image_path}\" style=\"max-width: 100%; height: 500px;\" /></div>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed2ff0-354a-4e12-803a-453f6fa3f041",
   "metadata": {},
   "source": [
    "### **Automated Gender Classification Using Facial Recognition and Advanced Deep Learning Techniques**\n",
    "\n",
    "### **Data Preprocessing**\n",
    "   - Image Augmentation For Training\n",
    "   - Image Augmentation For Testing\n",
    "### **Bulinding CNN Model**\n",
    "   - Conv2 layer\n",
    "   - MaxPooling\n",
    "   - Flatten\n",
    "   - Full Contection\n",
    "### üß† CNN Model Architecture  \n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input 64x64x3] --> B[Conv2D 32@3x3]\n",
    "    B --> C[ReLU]\n",
    "    C --> D[MaxPooling 2x2]\n",
    "    D --> E[Conv2D 64@3x3]\n",
    "    E --> F[ReLU]\n",
    "    F --> G[MaxPooling 2x2]\n",
    "    G --> H[Flatten]\n",
    "    H --> I[Dense 128]\n",
    "    I --> J[ReLU]\n",
    "    J --> K[Dense 2]\n",
    "    K --> L[sigmoid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25ed32ec-fd1c-48c8-8c5e-ee34b59a6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9a15270-9def-4b71-ae41-0c2904f50787",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_train=ImageDataGenerator(rescale=1./255,rotation_range=20,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "872640ef-9767-4eb5-8d59-8911a015dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47009 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "df_train=Aug_train.flow_from_directory('Training',target_size=(64,64),batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed3083f9-9941-40db-9256-d95d5a853e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "AugTest=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "523c323d-f7f4-44b0-a3a4-5bfd70451ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11649 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "df_test=AugTest.flow_from_directory('Validation',target_size=(64,64),batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0b632-35ab-4797-8afd-1bed1a202f16",
   "metadata": {},
   "source": [
    "## **CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ff63ad2-8170-4a90-805e-e7e224331a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN=tf.keras.models.Sequential([\n",
    "     # Convolutional layers\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=[64,64,3]),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(2, activation=\"sigmoid\"),  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a4ffe68-dd82-43eb-a656-c0697d4ce754",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6920c68-0eb4-4f7e-82e7-63d07dfc573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 228/1470\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m5:09\u001b[0m 249ms/step - accuracy: 0.5443 - loss: 0.6787"
     ]
    }
   ],
   "source": [
    "history = CNN.fit(x=df_train, validation_data=df_test, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784db29f-1b0a-4863-b488-677670adee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99abf3-ca3e-4c37-a78a-e7fa906c20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e4871-9b50-43ed-b28d-0ec00c79c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN.save('Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
